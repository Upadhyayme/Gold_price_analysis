{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d5cbdd12-c433-4c21-9651-dbb83d3ccb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-3.0.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\upadh\\onedrive\\documents\\custom office templates\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\upadh\\onedrive\\documents\\custom office templates\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-3.0.2-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/150.0 MB 2.8 MB/s eta 0:00:54\n",
      "   ---------------------------------------- 1.0/150.0 MB 2.4 MB/s eta 0:01:03\n",
      "   ---------------------------------------- 1.8/150.0 MB 2.8 MB/s eta 0:00:54\n",
      "    --------------------------------------- 2.6/150.0 MB 3.1 MB/s eta 0:00:48\n",
      "    --------------------------------------- 3.7/150.0 MB 3.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 4.5/150.0 MB 3.5 MB/s eta 0:00:42\n",
      "   - -------------------------------------- 5.2/150.0 MB 3.6 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 6.3/150.0 MB 3.8 MB/s eta 0:00:38\n",
      "   - -------------------------------------- 7.3/150.0 MB 3.8 MB/s eta 0:00:38\n",
      "   -- ------------------------------------- 8.7/150.0 MB 4.1 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 10.0/150.0 MB 4.3 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 11.3/150.0 MB 4.5 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 12.6/150.0 MB 4.6 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 14.2/150.0 MB 4.8 MB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 16.0/150.0 MB 5.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 17.8/150.0 MB 5.3 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 19.4/150.0 MB 5.5 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 21.5/150.0 MB 5.7 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 23.3/150.0 MB 5.8 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 24.9/150.0 MB 5.9 MB/s eta 0:00:22\n",
      "   ------- -------------------------------- 26.7/150.0 MB 6.1 MB/s eta 0:00:21\n",
      "   ------- -------------------------------- 28.6/150.0 MB 6.2 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 30.4/150.0 MB 6.3 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 32.0/150.0 MB 6.3 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 34.1/150.0 MB 6.5 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 35.7/150.0 MB 6.5 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 37.2/150.0 MB 6.6 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 39.3/150.0 MB 6.6 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 41.2/150.0 MB 6.7 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 43.0/150.0 MB 6.8 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 44.8/150.0 MB 6.9 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 45.9/150.0 MB 6.9 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 48.0/150.0 MB 6.9 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 49.8/150.0 MB 6.9 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 51.6/150.0 MB 6.9 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 53.5/150.0 MB 7.0 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 55.1/150.0 MB 7.0 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 56.6/150.0 MB 7.0 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 58.5/150.0 MB 7.0 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 60.0/150.0 MB 7.0 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 61.6/150.0 MB 7.1 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 63.2/150.0 MB 7.1 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 65.0/150.0 MB 7.1 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 66.6/150.0 MB 7.1 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 68.4/150.0 MB 7.2 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 70.3/150.0 MB 7.2 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 71.8/150.0 MB 7.2 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 73.7/150.0 MB 7.2 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 75.2/150.0 MB 7.2 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 76.8/150.0 MB 7.2 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 78.4/150.0 MB 7.2 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 80.0/150.0 MB 7.2 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 81.8/150.0 MB 7.2 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 83.4/150.0 MB 7.3 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 85.2/150.0 MB 7.3 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 86.5/150.0 MB 7.3 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 88.3/150.0 MB 7.3 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 90.2/150.0 MB 7.3 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 92.0/150.0 MB 7.3 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 93.6/150.0 MB 7.4 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 95.2/150.0 MB 7.4 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 96.7/150.0 MB 7.4 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 98.6/150.0 MB 7.4 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 100.4/150.0 MB 7.4 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 102.0/150.0 MB 7.4 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 103.8/150.0 MB 7.4 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 105.4/150.0 MB 7.4 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 106.7/150.0 MB 7.4 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 108.5/150.0 MB 7.4 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 110.4/150.0 MB 7.4 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 112.2/150.0 MB 7.4 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 114.0/150.0 MB 7.5 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 115.6/150.0 MB 7.5 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 116.9/150.0 MB 7.5 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 118.8/150.0 MB 7.5 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 120.6/150.0 MB 7.5 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 122.2/150.0 MB 7.5 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 123.7/150.0 MB 7.5 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 125.3/150.0 MB 7.5 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 127.1/150.0 MB 7.5 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 129.0/150.0 MB 7.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 130.8/150.0 MB 7.5 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 132.6/150.0 MB 7.5 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 134.5/150.0 MB 7.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 136.1/150.0 MB 7.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 137.9/150.0 MB 7.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 139.7/150.0 MB 7.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 141.3/150.0 MB 7.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 142.9/150.0 MB 7.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 144.7/150.0 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  146.3/150.0 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  147.6/150.0 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.4/150.0 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 150.0/150.0 MB 7.5 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.2\n",
      "Collecting eli5\n",
      "  Downloading eli5-0.16.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: attrs>17.1.0 in c:\\users\\upadh\\onedrive\\documents\\custom office templates\\lib\\site-packages (from eli5) (23.1.0)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in c:\\users\\upadh\\onedrive\\documents\\custom office templates\\lib\\site-packages (from eli5) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\upadh\\onedrive\\documents\\custom office templates\\lib\\site-packages (from eli5) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\upadh\\onedrive\\documents\\custom office templates\\lib\\site-packages (from eli5) (1.13.1)\n",
      "Collecting scikit-learn>=1.6.0 (from eli5)\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting graphviz (from eli5)\n",
      "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\upadh\\onedrive\\documents\\custom office templates\\lib\\site-packages (from eli5) (0.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\upadh\\onedrive\\documents\\custom office templates\\lib\\site-packages (from jinja2>=3.0.0->eli5) (2.1.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\upadh\\onedrive\\documents\\custom office templates\\lib\\site-packages (from scikit-learn>=1.6.0->eli5) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\upadh\\onedrive\\documents\\custom office templates\\lib\\site-packages (from scikit-learn>=1.6.0->eli5) (3.5.0)\n",
      "Downloading eli5-0.16.0-py2.py3-none-any.whl (108 kB)\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/10.7 MB 4.2 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.6/10.7 MB 4.2 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.4/10.7 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.4/10.7 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.5/10.7 MB 4.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.8/10.7 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.6/10.7 MB 4.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.3/10.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.1/10.7 MB 4.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.9/10.7 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.0/10.7 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: graphviz, scikit-learn, eli5\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.5.1\n",
      "    Uninstalling scikit-learn-1.5.1:\n",
      "      Successfully uninstalled scikit-learn-1.5.1\n",
      "Successfully installed eli5-0.16.0 graphviz-0.21 scikit-learn-1.7.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd  \n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install xgboost\n",
    "!pip install eli5\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\",\"grid.linestyle\": \":\"})\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5e840a6e-0bd4-4132-b7d9-6558c63ad469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e25d7-7935-4a91-9bde-39677edcf0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset using pandas function\n",
    "# use parse_dates argument to change datetime dtype\n",
    "data = pd.read_csv(\"C:/Users/upadh/Downloads/gold_price_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377114f4-b625-4d76-a4e9-6108b7a536b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about the dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd1926-cd23-43c8-b704-bc78454266a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values/Null Values Count\n",
    "data.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b31711-c565-4d89-bc83-e3cf2c41a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation = data.select_dtypes(include='number').corr()\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(correlation, cmap='coolwarm',center=0, annot=True)\n",
    "\n",
    "\n",
    "# Set title and axis labels\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4466f5c5-7807-41b6-b8b8-5bf722c3fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop SlV column\n",
    "data.drop(\"SLV\", axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b033b12-0c5b-4d1e-aa99-ae253b473542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot price of gold for each increasing day\n",
    "data[\"EUR/USD\"].plot()\n",
    "plt.title(\"Change in price of gold through date\")\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84cac4b-8451-4eb0-a1c9-fa5a7dacace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply rolling mean\n",
    "data[\"price_trend\"] = data[\"EUR/USD\"] / data[\"EUR/USD\"].rolling(window=20).mean()\n",
    "\n",
    "# Reset index (if 'date' was the index)\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "# Plotting price trend after removing first 20 NaNs\n",
    "data.loc[20:, \"price_trend\"].plot()\n",
    "\n",
    "# Set title and labels\n",
    "plt.title(\"Trend in price of EUR/USD through date\")\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"price_trend\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d6bdf-9976-42ae-a346-9cd60700bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# suptitle of the graph\n",
    "fig.suptitle('Distribution of data across column')\n",
    "temp = data.drop(\"Date\", axis=1).columns.tolist()\n",
    "for i, item in enumerate(temp):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.histplot(data=data, x=item, kde=True)\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b409db3-72bc-4ba2-9c37-4bdc7ace73af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewness along the index axis\n",
    "print(data.drop(\"Date\", axis=1).skew(axis=0, skipna=True))\n",
    "\n",
    "# This code is modified by shreya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b6ad8-4ab4-40ec-adb8-1d354a3b4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply saquare root transformation\n",
    "# on the skewed dataset\n",
    "data[\"USO\"] = data[\"USO\"] / np.sqrt(data[\"USO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede05229-f8d2-45e1-ad16-1688225b67c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "temp = data.drop(\"Date\", axis=1).columns.tolist()\n",
    "\n",
    "for i, item in enumerate(temp):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.boxplot(data=data, x=item, color='violet')\n",
    "\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72942346-1ece-4d4f-bbf4-a02679d588ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_removal(column):\n",
    "    # Capping the outlier rows with Percentiles\n",
    "    upper_limit = column.quantile(.95)\n",
    "    # set upper limit to 95percentile\n",
    "    lower_limit = column.quantile(.05)\n",
    "    # set lower limit to 5 percentile\n",
    "    column.loc[(column > upper_limit)] = upper_limit\n",
    "    column.loc[(column < lower_limit)] = lower_limit\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed61d6c-9a53-4f9f-9288-2f9342573cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize outliers in columns except Date\n",
    "\n",
    "data[['SPX', 'GLD', 'USO', 'EUR/USD']] = \\\n",
    "    data[['SPX', 'GLD', 'USO', 'EUR/USD']].apply(outlier_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c029ad-61ec-4ce8-b2ce-a22f60d8c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELING THE DATA\n",
    "# select the features and target variable\n",
    "X = data.drop(['Date', 'EUR/USD'], axis=1)\n",
    "\n",
    "y = data['EUR/USD']\n",
    "# dividing dataset in to train test\n",
    "x_train, x_test,\\\n",
    "    y_train, y_test = train_test_split(X, y,\n",
    "                                       test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f9bfe2-a703-4a2b-812d-f05f0b6fa961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALING THE DATA\n",
    "# Create an instance of the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler on the training dataset\n",
    "scaler.fit(x_train)\n",
    "\n",
    "# Transform the training dataset\n",
    "# using the StandardScaler\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb151a4c-542b-44de-a9b0-263334405784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LASSO REGRESSION\n",
    "\n",
    "# Impute missing values using SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean') # Replace NaNs with the mean of each column\n",
    "\n",
    "# Fit and transform the imputer on the scaled training data\n",
    "x_train_scaled = imputer.fit_transform(x_train_scaled)\n",
    "\n",
    "# Transform the scaled test data using the trained imputer\n",
    "x_test_scaled = imputer.transform(x_test_scaled)\n",
    "\n",
    "# Create a PolynomialFeatures object of degree 2\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Create a Lasso object\n",
    "lasso = Lasso()\n",
    "\n",
    "# Define a dictionary of parameter\n",
    "#values to search over\n",
    "param_grid = {'lasso__alpha': [1e-4, 1e-3, 1e-2,\n",
    "\t\t\t\t\t\t\t1e-1, 1, 5, 10, \n",
    "\t\t\t\t\t\t\t20, 30, 40]}\n",
    "\n",
    "# Create a pipeline that first applies \n",
    "# polynomial features and then applies Lasso regression\n",
    "pipeline = make_pipeline(poly, lasso)\n",
    "\n",
    "# Create a GridSearchCV object with \n",
    "#the pipeline and parameter grid\n",
    "lasso_grid_search = GridSearchCV(pipeline,param_grid, scoring='r2', cv=3)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "lasso_grid_search.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict the target variable using\n",
    "# the fitted model and the test data\n",
    "y_pred = lasso_grid_search.predict(x_train_scaled)\n",
    "\n",
    "# Compute the R-squared of the fitted model on the train data\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "\n",
    "# Print the R-squared\n",
    "print(\"R-squared: \", r2)\n",
    "\n",
    "# Print the best parameter values and score\n",
    "print('Best parameter values: ',lasso_grid_search.best_params_)\n",
    "print('Best score: ',lasso_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a36f64a-19ea-41c3-8cd2-3bf1a97c505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST REGRESSOR FOR REGRESSION\n",
    "\n",
    "# Insiate param grid for which to search\n",
    "param_grid = {'n_estimators': [50, 80, 100],\n",
    "              'max_depth': [3, 5, 7]}\n",
    "\n",
    "# create instance of the Randomforest regressor\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Define Girdsearch with random forest\n",
    "# object parameter grid scoring and cv\n",
    "rf_grid_search = GridSearchCV(rf, param_grid, scoring='r2', cv=2)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "\n",
    "rf_grid_search.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameter values and score\n",
    "print('Best parameter values: ', rf_grid_search.best_params_)\n",
    "print('Best score: ', rf_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e9a9c-07cf-4a8a-b4de-fa68a3e00b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the R-squared of the\n",
    "# fitted model on the test data\n",
    "r2 = r2_score(y_test,\n",
    "              rf_grid_search.predict(x_test_scaled))\n",
    "\n",
    "# Print the R-squared\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19d1b8-7039-4d24-a74e-7cda8176ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop(\"Date\", axis=1).columns\n",
    "\n",
    "# store the importance of the feature\n",
    "importances = rf_grid_search.best_estimator_.\\\n",
    "    feature_importances_\n",
    "\n",
    "\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# title of the graph\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "plt.barh(range(len(indices)),\n",
    "         importances[indices],\n",
    "         color='red',\n",
    "         align='center')\n",
    "\n",
    "# plot bar chart\n",
    "plt.yticks(range(len(indices)),\n",
    "           [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72993cb1-486e-4062-a0c7-6f705b36dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost Model for Regression \n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Create an instance of the XGBRegressor model\n",
    "model_xgb = XGBRegressor()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_xgb.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict on training data\n",
    "y_train_pred = model_xgb.predict(x_train_scaled)\n",
    "\n",
    "# Print the R-squared score on the training data\n",
    "print(\"XGBoost R^2 Score on Training Data =\", r2_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3632cf1-c96a-4bd3-a761-e1759a5692ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the R-squared score on the test data\n",
    "print(\"Xgboost Accuracy on test data =\",\n",
    "      r2_score(y_test, model_xgb.predict(x_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdbd351-3d21-431c-b267-80757fe60f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Explainability \n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance  # Optional, for more detailed insights\n",
    "\n",
    "# Make sure your model is using the scikit-learn API\n",
    "# For example: model_xgb = xgb.XGBClassifier().fit(x_train, y_train)\n",
    "\n",
    "# Explain weights (feature importances)\n",
    "eli5.show_weights(model_xgb, feature_names=x_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdba272-359b-4a1c-937d-d51b8d4de293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Deloyment using Pickle\n",
    "\n",
    "# dump model using pickle library\n",
    "import pickle\n",
    "\n",
    "# dump model in file model.pkl\n",
    "with open('model.pkl', 'wb') as file:\n",
    "pickle.dump(model_xgb,file)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
